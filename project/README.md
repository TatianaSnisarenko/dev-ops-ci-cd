# üß± Lesson 7 ‚Äî EKS + ECR + Helm + HPA (Django on AWS)

This lesson provisions an EKS cluster with Terraform, pushes a Django Docker image to ECR, and deploys the app to the cluster via a Helm chart with Horizontal Pod Autoscaler (HPA).

## üéØ Objectives

- Provision EKS in an existing VPC using Terraform modules.
- Build and push the Django image to Amazon ECR.
- Provision a PostgreSQL database using a **universal RDS module** (standard RDS instance or Aurora cluster via `use_aurora`).
- Deploy the app with a Helm chart (Deployment, Service, ConfigMap, Secret, HPA).
- Verify scaling with HPA (CPU-based).
- (Bonus) Add Ingress + TLS via cert-manager.

## üóÇÔ∏è Project Structure

```
dev-ops-ci-cd/
‚îÇ
‚îú‚îÄ‚îÄ django/
‚îÇ   ‚îú‚îÄ‚îÄ manage.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ myproject/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ asgi.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ urls.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py
‚îÇ   ‚îî‚îÄ‚îÄ .env.example
‚îÇ
‚îú‚îÄ‚îÄ project/
‚îÇ   ‚îú‚îÄ‚îÄ charts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ django-app/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ values.yaml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ values.secret.local.yaml # gitignored
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ _helpers.tpl
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ secret.yaml
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ service.yaml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backend.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfstate
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfstate.backup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ s3-backend/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ s3.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dynamodb.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vpc/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ vpc.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ routes.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ecr/
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ ecr.tf
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ       |   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ eks/
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ eks.tf
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ       |   ‚îî‚îÄ‚îÄ outputs.tf
|   |       ‚îú‚îÄ‚îÄ jenkins/
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ jenkins.tf
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ providers.tf
‚îÇ   ‚îÇ       |   ‚îú‚îÄ‚îÄ values.yaml
‚îÇ   ‚îÇ       |   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ       |
‚îÇ   |       ‚îî‚îÄ‚îÄ argo_cd/             # ‚úÖ –ù–æ–≤–∏–π –º–æ–¥—É–ª—å –¥–ª—è Helm-—É—Å—Ç–∞–Ω–æ–≤–∫–∏ Argo CD
‚îÇ   |           ‚îú‚îÄ‚îÄ argo_cd.tf       # Helm release –¥–ª—è Jenkins
‚îÇ   |           ‚îú‚îÄ‚îÄ variables.tf     # –ó–º—ñ–Ω–Ω—ñ (–≤–µ—Ä—Å—ñ—è —á–∞—Ä—Ç–∞, namespace, repo URL —Ç–æ—â–æ)
‚îÇ   |           ‚îú‚îÄ‚îÄ providers.tf     # Kubernetes+Helm.  –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –∑ –º–æ–¥—É–ª—è jenkins
‚îÇ   |           ‚îú‚îÄ‚îÄ values.yaml      # –ö–∞—Å—Ç–æ–º–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Argo CD
‚îÇ   |           ‚îú‚îÄ‚îÄ outputs.tf       # –í–∏–≤–æ–¥–∏ (hostname, initial admin password)
‚îÇ	 |          ‚îî‚îÄ‚îÄcharts/                  # Helm-—á–∞—Ä—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è app'—ñ–≤
‚îÇ 	 |	            ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ	 |	            ‚îú‚îÄ‚îÄ values.yaml          # –°–ø–∏—Å–æ–∫ applications, repositories
‚îÇ	 |	            ‚îî‚îÄ‚îÄ templates/
‚îÇ	 |                  ‚îú‚îÄ‚îÄ application.yaml
‚îÇ	 |                  ‚îî‚îÄ‚îÄ repository.yaml
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îú‚îÄ‚îÄ backend.tf
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ install_dev_tools.sh
‚îî‚îÄ‚îÄ README.md

```

### üóÑÔ∏è Module: `rds` (Universal RDS / Aurora PostgreSQL)

This module provisions the database layer for the Django application.

It supports two modes controlled by the `use_aurora` flag:

- `use_aurora = false` ‚Üí **standard RDS instance** (`aws_db_instance`)
- `use_aurora = true` ‚Üí **Aurora PostgreSQL cluster** (`aws_rds_cluster` + `aws_rds_cluster_instance` writer + replicas)

In both modes the module automatically creates:

- **DB Subnet Group** (`aws_db_subnet_group`) ‚Äî uses private or public subnets depending on `publicly_accessible`
- **Security Group** (`aws_security_group`) ‚Äî allows PostgreSQL port (5432) from the VPC CIDR
- **Parameter Group**:
  - `aws_db_parameter_group` for standard RDS
  - `aws_rds_cluster_parameter_group` for Aurora

#### Key inputs

- `name` ‚Äî base name for DB resources
- `use_aurora` ‚Äî switch between standard RDS and Aurora
- `engine`, `engine_version`, `parameter_group_family_rds` ‚Äî RDS engine settings (e.g. `postgres`, `17.2`, `postgres17`)
- `engine_cluster`, `engine_version_cluster`, `parameter_group_family_aurora`, `aurora_replica_count` ‚Äî Aurora engine settings (e.g. `aurora-postgresql`, `15.3`, `aurora-postgresql15`)
- `instance_class`, `allocated_storage`, `multi_az`
- `db_name`, `username`, `password`
- `vpc_id`, `subnet_private_ids`, `subnet_public_ids`, `publicly_accessible`, `vpc_cidr_block`
- `parameters` ‚Äî `map(string)` with DB parameters (e.g. `max_connections`, `log_min_duration_statement`)
- `backup_retention_period`, `tags`

#### Outputs

- `endpoint` ‚Äî RDS instance address **or** Aurora cluster endpoint
- `port` ‚Äî database port
- `db_name`
- `master_username`
- `master_password`
- `security_group_id` ‚Äî Security Group attached to the DB

These outputs are used in the root module to create a Kubernetes Secret `django-db`, which injects DB connection settings into the Django application.

## ‚öôÔ∏è Terraform Modules Overview

### üóëÔ∏è Module: `s3-backend`

Creates:

- **S3 bucket** for storing Terraform state files
- **DynamoDB table** for state locking
- Bucket versioning and encryption enabled

Outputs:

- S3 bucket name
- DynamoDB table name

### üåê Module: `vpc`

Creates:

- **VPC** with CIDR block
- 3 public and 3 private subnets across different Availability Zones
- **Internet Gateway (IGW)** for public subnets
- **NAT Gateways** for private subnets
- **Route tables** for traffic routing

Outputs:

- VPC ID
- Public and private subnet IDs

### üì¶ Module: `ecr`

Creates:

- **ECR repository** with image scanning on push
- Configures encryption and tagging

Outputs:

- Repository URL

‚ò∏Ô∏è Module: `eks`

- Module: terraform-aws-modules/eks/aws
- Kubernetes version: 1.29 (target)

Features

- Public API endpoint (cluster endpoint accessible for kubectl)
- Managed Node Group deployed into private subnets

Managed Node Group (example settings)

- instance_types: ["t3.small"]
- desired_size: 1
- min_size: 1
- max_size: 2

Outputs

- eks_cluster_name
- eks_cluster_endpoint
- eks_node_group_name
- eks_oidc_provider_arn

Note

- With nodes in private subnets, NAT Gateways are required so nodes can pull images from ECR and access the internet securely.

...existing code...

## üì¶ Helm Chart Overview (charts/django-app)

- deployment.yaml ‚Äî uses the ECR image and envFrom ConfigMap/Secret
- service.yaml ‚Äî LoadBalancer exposure (per task requirements)
- configmap.yaml ‚Äî non-secret env (DB host/port/name, etc.)
- secret.yaml ‚Äî sensitive env injected from values.secret.local.yaml (gitignored)
- hpa.yaml ‚Äî CPU-based autoscaling (default 70%, min 1, max 3)
- values.yaml ‚Äî image tag, service type/port, autoscaling params, optional ingress

Secrets file example (gitignored: values.secret.local.yaml):

```yaml
secret:
  enabled: true
  existingName: ""
  data:
    POSTGRES_USER: "django_user"
    POSTGRES_PASSWORD: "django_password"
```

## üß© Backend Configuration

Terraform backend stores the project‚Äôs state remotely in AWS S3 and uses DynamoDB for locking to prevent parallel modifications.

## üîÑ Terraform Workflow and Remote Backend

This project uses a **remote backend (S3 + DynamoDB)** to safely store and lock Terraform state files.
All Terraform commands operate on this centralized remote state to avoid configuration drift when working in teams.

The standard workflow is:

1. `terraform init` ‚Äì connect to backend and download providers
2. `terraform plan -out=tfplan` ‚Äì generate a plan of upcoming changes
3. `terraform apply tfplan` ‚Äì apply the exact plan
4. `terraform destroy` ‚Äì tear down the infrastructure

This guarantees repeatability and prevents concurrent state modifications.

##‚úÖ Prerequisites

- Terraform 1.13.x (or >= 1.9)
- AWS CLI v2 configured with profile: terraform
- kubectl (compatible with K8s 1.29)
- Helm 3.x
- Docker (to build & push image)
- AWS Region: eu-north-1
- AWS CLI profile: terraform

Quick checks:

```bash
terraform version
aws sts get-caller-identity --profile terraform
kubectl version --client
helm version
docker version
```

## üîê AWS Access Configuration for Terraform

### 1. Create a dedicated IAM user `terraform`

For security reasons, Terraform should **not** run using the root account or an admin user.  
Instead, create a separate IAM user with only the permissions required for managing infrastructure.

#### Steps:

### üß© Step 1 ‚Äî Base group: `terraform-lab`

This group contains basic permissions for Terraform backend and networking.

1. In the AWS Console go to **IAM ‚Üí User groups ‚Üí Create group**.  
   Name the group: `terraform-lab`

2. Attach the following AWS managed policies to the group:

   - `AmazonS3FullAccess` ‚Äî for storing Terraform state in S3
   - `AmazonDynamoDBFullAccess` ‚Äî for Terraform state locking (use this name; avoid a non-standard `_v2` suffix)
   - `AmazonEC2FullAccess` ‚Äî for creating EC2 resources, VPC, subnets, NAT, routing
   - `AmazonEC2ContainerRegistryFullAccess` ‚Äî for building and pushing Docker images to ECR

3. Create IAM user `terraform`:

   - Access type: **Programmatic access (CLI/API only)**
   - Add user `terraform` to group `terraform-lab`

4. Generate Access Keys for CLI use:
   - IAM ‚Üí Users ‚Üí `terraform` ‚Üí **Security credentials** ‚Üí **Create access key** ‚Üí _Application running outside AWS_
   - Save `Access key ID` and `Secret access key` securely ‚Äî used by the Terraform AWS provider.

---

### üèó Step 2 ‚Äî Provisioning group: `terraform-provisioners`

This group contains additional permissions required to provision EKS and related resources.

1. In IAM ‚Üí User groups ‚Üí Create group, name it `terraform-provisioners`.

2. Attach these AWS managed policies:

   - `AmazonVPCFullAccess` ‚Äî for networking components used by EKS
   - `CloudWatchFullAccess` (or the appropriate CloudWatch managed policy/version for your account) ‚Äî for EKS log groups
   - `IAMFullAccess` ‚Äî to create IAM roles and attach policies used by the EKS module
   - `AWSKeyManagementServicePowerUser` ‚Äî optional, only if EKS cluster encryption with KMS is enabled

3. Attach a customer-managed policy `TerraformEKSProvision` (create this beforehand) which should include at least:

   - `eks:*` for managing clusters and node groups (or scoped least-privilege equivalents)
   - `iam:PassRole` limited to EKS-related roles
   - Access to S3 and DynamoDB backend resources used by Terraform
   - Minimal CloudWatch Logs permissions

4. Add the same IAM user `terraform` to this group:
   - IAM ‚Üí Users ‚Üí `terraform` ‚Üí **Groups** ‚Üí Add to `terraform-provisioners`

Notes

- Prefer least-privilege policies in production; the above managed policies are convenient for labs.
- Verify the user via `aws sts get-caller-identity --profile terraform` after configuring the AWS CLI.

### üèó Step 3

Generate **Access Keys** for this user:

- Navigate to `IAM ‚Üí Users ‚Üí terraform ‚Üí Security credentials`
- Click **Create access key**
- Choose _Application running outside AWS_
- Save the `Access key ID` and `Secret access key` securely.

### 2. Configure AWS CLI Profile (Windows)

1. Install AWS CLI if not already installed:

```powershell
choco install awscli

```

2. Configure a new CLI profile named terraform:

```
aws configure --profile terraform
```

Enter the values:

```
AWS Access Key ID [None]: <your_access_key>
AWS Secret Access Key [None]: <your_secret_key>
Default region name [None]: eu-north-1
Default output format [None]: json
```

Verify the connection:

```
aws sts get-caller-identity --profile terraform
```

You should see your AWS Account ID and the ARN of the terraform user.

### 3. Using the profile in Terraform

In your Terraform configuration, specify the created profile:

```hcl
provider "aws" {
region = "eu-north-1"
profile = "terraform"
}
```

Terraform will automatically load credentials from this AWS CLI profile when executing commands.

This setup uses:

- **AWS region:** `eu-north-1`
- **AWS CLI profile:** `terraform`

Make sure that the same region and profile are configured in your AWS CLI before running any Terraform commands.

4. Validation and Initialization

Before deployment, run:

```bash
terraform init
terraform plan
terraform apply
```

Terraform will authenticate through your terraform IAM user using the terraform profile, ensuring secure and isolated access to AWS resources without requiring admin privileges.

## üöÄ How to Run

This project provisions AWS infrastructure with Terraform (remote state in S3 + DynamoDB), installs Jenkins and Argo CD using Helm (via Terraform), builds Docker images via Jenkins, and deploys applications to EKS through Argo CD using GitOps.

**The CI/CD flow now looks like this:**

- Terraform ‚Üí provisions infrastructure (VPC, EKS, RDS, ECR, Jenkins, Argo CD).
- Jenkins ‚Üí builds and pushes Docker images to ECR + updates Helm chart versions in Git.
- Argo CD ‚Üí watches the Git repository and automatically syncs updated Helm charts into EKS.

### 0Ô∏è‚É£ Prerequisites

Before running the full stack:

- AWS CLI configured (profile = terraform)
- kubectl installed
- Terraform installed
- Docker installed (for initial sanity checks)
- GitHub repository accessible (public, or private with Jenkins credentials)

### 1Ô∏è‚É£ Comment out the backend

Before the first run, open `\lesson-7\backend.tf` and temporarily disable the backend block:

```hcl
# terraform {
#   backend "s3" {
#     bucket         = "terraform-state-bucket-a3f7d92c"
#     key            = "lab/terraform.tfstate"
#     region         = "eu-north-1"
#     dynamodb_table = "terraform-locks"
#     encrypt        = true
#     profile        = "terraform"
#   }
# }
```

Terraform cannot initialize directly to an S3 backend if the bucket and DynamoDB table don‚Äôt exist yet.
This step ensures that state is handled locally until backend resources are provisioned.

### 2Ô∏è‚É£ Initialize and apply locally

```
cd project
terraform init
terraform validate

# create repo files and update cache (once)
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server
helm repo update

terraform plan -out=tfplan
terraform apply tfplan
```

‚úÖ This creates:

- S3 bucket for state + DynamoDB table for locking
- VPC (subnets, IGW/NAT, routes)
- ECR repository
- EKS cluster with a managed node group (+ admin bootstrap for creator)
- RDS PostgreSQL (private)
- Kubernetes Secret django-db (DB_HOST/PORT/NAME/USER/PASSWORD + DATABASE_URL) in the target namespace
- IAM roles/policies as needed

‚ö†Ô∏è Note: At this stage, Terraform still uses a **local state file (terraform.tfstate)**.  
The backend (S3 + DynamoDB) will be connected in the next step.

### 3Ô∏è‚É£ Re-enable the backend and migrate state to S3

Once all resources are created, uncomment the backend configuration in backend.tf:

```
terraform {
  backend "s3" {
    bucket         = "terraform-state-bucket-a3f7d92c"
    key            = "lab/terraform.tfstate"
    region         = "eu-north-1"
    dynamodb_table = "terraform-locks"
    encrypt        = true
    profile        = "terraform"
  }
}
```

Reconfigure Terraform to use the remote backend and migrate the local state to S3:

```
terraform init -reconfigure
# When prompted: "Do you want to copy existing state to the new backend?" -> yes
```

### 4Ô∏è‚É£ Validate backend connection

Run:

```
terraform state list
terraform plan
```

Expected output: **No changes**

In AWS Console:
S3 ‚Üí you should see the file lab/terraform.tfstate in bucket terraform-state-bucket-a3f7d92c
DynamoDB ‚Üí table terraform-locks will briefly show a LockID during Terraform operations

### 5Ô∏è‚É£ Update the Django Chart and Jenkinsfile With Your ECR URL (one-time step)

`After Terraform finishes:

```bash
terraform output -raw ecr_repository_url
```

Put this URL (example: 123456789012.dkr.ecr.eu-north-1.amazonaws.com/django-app) into:

project/charts/django-app/values.yaml:

```bash
image:
  repository: "<your-ecr-url>"
  tag: "v1"
  pullPolicy: IfNotPresent
```

\Jenkinsfile

```bash
ECR_REPOSITORY = "<your-ecr-url>"

```

Commit changes:

```
git add .
git commit -m "Set ECR image repository for Argo CD"
git push
```

Argo CD now knows where the image will come from.

## 6Ô∏è‚É£ Configure Jenkins Credentials (GitHub PAT)

Get url

```bash
kubectl get svc jenkins -n jenkins
```

Copy host name and open in browser

http://<—Ü–µ–π-hostname>/

Get password:

```bash
$pass64 = kubectl get secret jenkins -n jenkins -o jsonpath='{.data.jenkins-admin-password}'
$bytes  = [System.Convert]::FromBase64String($pass64)
$pass   = [System.Text.Encoding]::UTF8.GetString($bytes)
$pass
```

Use login = admin and pass to login into Jenkins UI

In Jenkins UI configure credentials:

Manage Jenkins ‚Üí Credentials ‚Üí Global ‚Üí Add Credentials

Type: Username + Password

- Username = your GitHub username
- Password = GitHub Personal Access Token (PAT)
- ID = github-token

This credential will be used in the Jenkins pipeline to push Helm chart updates back to Git.

## 7Ô∏è‚É£ Jenkins: Trigger Initial Seed Job

In Jenkins ‚Üí Dashboard ‚Üí Jobs: seed-job start manually with Build Now

If seed job fails - this may be caused by security reasons - you need to allow script running mannually:

In Jenkins Dashboard -> Manage Jenkins -> ScriptApproval -> Approve script

Re-run seed-job

Terraform has already installed Jenkins and applied its JCasC configuration.

Port-forward or use the LoadBalancer service:

```bash
kubectl -n jenkins get svc
```

If LoadBalancer:

Open URL ‚Üí log in (admin credentials from Jenkins output).

Run the Seed Job ‚Üí it will generate the main pipeline:

```bash
django-ci-cd
```

## 8Ô∏è‚É£ Full CI Pipeline (Jenkinsfile Execution)

When you run the django-ci-cd pipeline:

Jenkins will automatically:

- Build your Django Docker image using Kaniko
- Tag it with $BUILD_NUMBER or another version
- Push it to ECR
- Clone your GitOps repo
- Update:

```
charts/django-app/values.yaml ‚Üí image.tag: "new-tag"
```

- Commit and push the change
- Notify Argo CD via GitOps

## 9Ô∏è‚É£ Argo CD Automatically Syncs the Updated Chart

Get Argo CD URL:

```bash
kubectl -n argocd get svc argo-cd-argocd-server  -o wide
```

If LoadBalancer is used:

Open in browser ‚Üí log in:
username: admin
Password (Terraform output):

```bash
kubectl -n argocd get secret argocd-initial-admin-secret -o go-template="{{.data.password | base64decode}}"

```

Argo CD will detect Git changes and automatically:

- Pull the updated Helm chart
- Update Deployment in EKS
- Restart pods
- Sync HPA, ConfigMap, Secrets, etc.

Check rollout:

```bash
kubectl get pods -n default
kubectl get svc -n default
kubectl get deploy -n default
kubectl get hpa -n default
```

üîü Validate the Application

```bash
kubectl get svc -n default
```

Open the hostname in the browser ‚Üí Django app should respond.

### üîÅ Switching from standard RDS to Aurora (optional)

By default the configuration uses a **standard RDS PostgreSQL instance**:

```hcl
module "rds_postgres" {
  source = "./modules/rds"

  name       = "${var.cluster_name}-db"
  use_aurora = false

  engine                     = "postgres"
  engine_version             = "17.2"
  parameter_group_family_rds = "postgres17"

  # shared settings
  instance_class    = "db.t3.micro"
  allocated_storage = 20

  db_name  = var.db_name
  username = var.db_username
  password = random_password.rds_master.result

  vpc_id             = module.vpc.vpc_id
  subnet_private_ids = module.vpc.private_subnet_ids
  subnet_public_ids  = module.vpc.public_subnet_ids

  publicly_accessible = false
  multi_az            = false

  vpc_cidr_block          = var.vpc_cidr_block
  backup_retention_period = "0"

  parameters = {
    max_connections            = "200"
    log_min_duration_statement = "500"
  }
}
```

To switch this module to an Aurora PostgreSQL cluster, only a few values need to be changed:

```hcl
module "rds_postgres" {
  source = "./modules/rds"

  name       = "${var.cluster_name}-aurora"
  use_aurora = true

  # Aurora engine
  engine_cluster             = "aurora-postgresql"
  engine_version_cluster     = "15.3"
  parameter_group_family_aurora = "aurora-postgresql15"
  aurora_replica_count       = 1

  # RDS engine settings are ignored when use_aurora = true
  engine                     = "postgres"
  engine_version             = "17.2"
  parameter_group_family_rds = "postgres17"

  # shared settings
  instance_class    = "db.t3.medium"
  allocated_storage = 20 # ignored by Aurora

  db_name  = var.db_name
  username = var.db_username
  password = random_password.rds_master.result

  vpc_id             = module.vpc.vpc_id
  subnet_private_ids = module.vpc.private_subnet_ids
  subnet_public_ids  = module.vpc.public_subnet_ids

  publicly_accessible = false
  vpc_cidr_block      = var.vpc_cidr_block

  backup_retention_period = "0"

  parameters = {
    max_connections            = "200"
    log_min_duration_statement = "500"
  }
}
```

## Cleanup

To delete the deployment and test again later:

```bash
kubectl delete app django-app -n argocd
helm uninstall jenkins -n jenkins
```

üßπ Proper teardown when using an S3 backend (with DynamoDB locking)

Terraform cannot safely destroy its own remote backend if that backend is stored in an S3 bucket with versioning enabled. A simple `terraform destroy` may fail with:

- Error: `BucketNotEmpty` ‚Äî the S3 bucket still contains objects or versions.
- Error releasing the state lock ‚Äî DynamoDB lock table missing or unreadable.

Follow these steps to safely destroy all infrastructure, including backend resources.

üîß **_Step-by-step teardown procedure_**

1. Comment out the S3 backend block in `/lesson-7/backend.tf` so Terraform will use a local backend for the teardown.

```hcl
# terraform {
#   backend "s3" {
#     bucket         = "terraform-state-bucket-XXXX"
#     key            = "lab/terraform.tfstate"
#     region         = "eu-north-1"
#     dynamodb_table = "terraform-locks"
#     encrypt        = true
#     profile        = "terraform"
#   }
# }
```

2. Re-initialize Terraform to use a local backend:

```
terraform init -reconfigure
# If prompted to migrate the state, confirm with:
# terraform init -migrate-state -lock=false
```

Terraform will now keep the state file locally (terraform.tfstate).

3. Refresh the local state without changing resources:

```
terraform plan -refresh-only
terraform apply -refresh-only -auto-approve
```

This ensures that the local state reflects the actual resources currently existing in AWS.

4. Manually empty the S3 bucket (including all versions and delete markers). Using the AWS Console is simplest for versioned buckets:

- Open S3 ‚Üí your bucket ‚Üí Show versions ‚Üí select all objects & versions ‚Üí Delete ‚Üí Permanently delete.

5. Run final destroy:

```bash
terraform destroy -auto-approve
```

Now Terraform will cleanly remove:
All VPC, subnets, NAT gateways, etc.
The DynamoDB lock table (terraform-locks)
Any remaining AWS resources declared in your configuration.

8. Verify cleanup:

```bash
terraform state list
```

Command should show no remaining resources.

## üì∏ Screenshots (Verification Results)

### 1. S3 Bucket and DynamoDB Lock

![S3](./screenshots/s3.png)

![DynamoDB](./screenshots/dynamoDb.png)

### 2. VPC

![VPC Subnets](./screenshots/vpc.png)

### 3. ECR Repository

![ECR Repository](./screenshots/ecr.png)

### 4. HPA

![HPA](./screenshots/hpa.png)

### 5. Jenkins

![Jenkins](./screenshots/jenkins.png)

![Seed Job](./screenshots/seed-job.png)

![Django Ci Cd](./screenshots/django-ci-cd.png)

### 6. Argo CD

![Argo CD](./screenshots/argo_cd.png)

### 5. Django app

![Django app](./screenshots/django-app.png)

## üõ°Ô∏è Best Practices

- Use a dedicated IAM user for Terraform with minimal permissions.
- Store state remotely (S3 + DynamoDB) ‚Äî never commit terraform.tfstate to Git.
- Enable versioning and encryption on your S3 bucket.
- Use separate workspaces or backends for dev/stage/prod environments.
- Rotate access keys periodically.

## üß† Learning Outcomes

By completing this assignment, you will:

- Understand Terraform backend configuration and remote state management.
- Learn how to organize infrastructure into reusable modules.
- Practice creating AWS network resources (VPC, subnets, gateways).
- Deploy and manage container registry resources (ECR).
- Provision a production-like PostgreSQL database using a **universal RDS module** with support for **Aurora**.
- Integrate DB credentials into Kubernetes via Secrets and Helm charts.
- Apply real-world infrastructure-as-code and GitOps patterns used in professional DevOps workflows.
